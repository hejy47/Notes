# 深度强化学习（Deep Reinforcement Learning）

2021/12/15 学习笔记

## 原理

### 强化学习

在强化学习中，一个智能体在一定数量的离散时间步骤中与它的环境进行交互。

1. 在时间步骤$t$，环境向智能体表现一个状态$s_t\in \mathcal{S}$；
2. 智能体选择一个动作$a_t\in \mathcal{A}$作为回应，该选择由智能体策略来决定：
   $$
    \pi(a \mid s)=\operatorname{Pr}\left\{a_{t}=a \mid s_{t}=s\right\}
    $$
3. 该动作会反馈回环境，并且它的执行可能回修改环境的内在状态；
4. 智能体回获得更新的状态$s_{t+1}$和一个奖励$r_t$；
5. 智能体的目标是去最大化每个状态$s_t$的未来反馈的预期总和$G_t$：
   $$
    G_{t}=\sum_{k=0}^{T-t-1} \gamma^{k} r_{t+k}
    $$

### 回馈值函数

策略在状态s的长期期望回馈值函数:

$$
V_\pi (s) = E_\pi [G_t \mid S_t=s]
$$

策略在状态s，采取动作a的长期期望回馈值函数:
$$
Q_\pi (s,a) = E_\pi [G_t \mid S_t=s, A_t=a]
$$

### 深度强化学习

参考[链接](https://zhuanlan.zhihu.com/p/48867049#:~:text=%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88DRL%EF%BC%8Cdeep%20reinforcement%20learning%EF%BC%89%E6%98%AF%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9B%B8%E7%BB%93%E5%90%88%E7%9A%84%E4%BA%A7%E7%89%A9%EF%BC%8C%E5%AE%83%E9%9B%86%E6%88%90%E4%BA%86%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8%E8%A7%86%E8%A7%89%E7%AD%89%E6%84%9F%E7%9F%A5%E9%97%AE%E9%A2%98%E4%B8%8A%E5%BC%BA%E5%A4%A7%E7%9A%84%E7%90%86%E8%A7%A3%E8%83%BD%E5%8A%9B%EF%BC%8C%E4%BB%A5%E5%8F%8A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%86%B3%E7%AD%96%E8%83%BD%E5%8A%9B%EF%BC%8C%E5%AE%9E%E7%8E%B0%E4%BA%86%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AD%A6%E4%B9%A0%E3%80%82.%20%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%87%BA%E7%8E%B0%E4%BD%BF%E5%BE%97%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E7%9C%9F%E6%AD%A3%E8%B5%B0%E5%90%91%E5%AE%9E%E7%94%A8%EF%BC%8C%E5%BE%97%E4%BB%A5%E8%A7%A3%E5%86%B3%E7%8E%B0%E5%AE%9E%E5%9C%BA%E6%99%AF%E4%B8%AD%E7%9A%84%E5%A4%8D%E6%9D%82%E9%97%AE%E9%A2%98%E3%80%82.,%E4%BB%8E2013%E5%B9%B4DQN%EF%BC%88%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9C%EF%BC%8Cdeep%20Q%20network%EF%BC%89%E5%87%BA%E7%8E%B0%E5%88%B0%E7%9B%AE%E5%89%8D%E4%B8%BA%E6%AD%A2%EF%BC%8C%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%A2%86%E5%9F%9F%E5%87%BA%E7%8E%B0%E4%BA%86%E5%A4%A7%E9%87%8F%E7%9A%84%E7%AE%97%E6%B3%95%EF%BC%8C%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E9%97%AE%E9%A2%98%E7%9A%84%E8%AE%BA%E6%96%87%E3%80%82.%20%E5%9C%A8%E8%BF%99%E7%AF%87%E6%96%87%E7%AB%A0%E4%B8%AD%EF%BC%8CSIGAI%E5%B0%86%E5%AF%B9%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%AE%97%E6%B3%95%E4%B8%8E%E5%BA%94%E7%94%A8%E8%BF%9B%E8%A1%8C%E6%80%BB%E7%BB%93%E3%80%82.)

## DeepRL与GP的区别

参考[链接](https://blog.csdn.net/deephub/article/details/119168308)

## 如何将DeepRL替换成GP

考虑RL四元组：$<A,S,R,P>$，其中$A$指智能体的所有修复操作，$S$指智能体所能感知的环境状态即程序状态，$R$指智能体执行动作后得到的回馈，$P$指智能体执行动作后程序状态的更新。

那么，$A$指的就是GP中的代码修改操作（包括交叉变异），$R$指的就是GP中的适应度值，$P$指的是修改后程序的更新。

## DeepRL在自动修复上的应用

融入DeepRL到补丁生成需要考虑以下三个问题：

1. 现有进化算法对可疑语句进行变异、重组等修复操作时，其修复操作过于简单（如：随机作插入、删除、替换操作等）。拟基于可疑性语义场景，采用基于模板或基于代码复用的方法，以此完成合理的修复操作设计，从而生成更高质量的补丁。
2. 现有的回馈值函数（适应度函数）过于简单，指导性不足。项目组拟结合更多缺陷相关信息（如：测试语义信息、运行域诸如断言、堆栈等信息），以此设计出合理的回馈值函数，提供更有用修复动作策略选择的指导性。
3. 现有进化算法修复操作策略选择过于简单（如：遗传算法随机选择一个修复操作进行补丁生成）。考虑深度学习在策略函数拟合上具有强大的表征能力，拟基于可疑性语义场景，采用基于深度神经网络的方法，以此完成符合语义场景的最优策略设计，从而生成更高质量的补丁。

